{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from scipy import optimize\n",
    "sys.path.insert(1,\"../ai_implementations\")\n",
    "import custom_ai_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the testing data\n",
    "data_path = \"data/intro.csv\"\n",
    "data = np.loadtxt(data_path, delimiter = \",\", usecols = (1,2,3,4,5,6))\n",
    "\n",
    "# Partition training data and result data \n",
    "X = data[:,:5]\n",
    "y = data[:,5]\n",
    "m = y.size\n",
    "\n",
    "# Add the intercept term\n",
    "X = np.concatenate([np.ones((m, 1)), X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costGradRegFunction(theta, X, y, lambda_, powers):\n",
    "    \"\"\"\n",
    "        Perform one step of regularized gradient descent\n",
    "        \n",
    "        Raise X to powers and apply theta, then regularize with lambda_ and compare to y\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : numpy array (6,)\n",
    "            The weights to be applied to X\n",
    "        X : numpy array (n,6)\n",
    "            The feature vector\n",
    "        y : numpy array (n,)\n",
    "            The result vector, containing actual voted outcomes for each feature vector\n",
    "        lambda_ : float\n",
    "            The regularization parameter\n",
    "        powers : array (6,)\n",
    "            The exponents to raise each feature to in X, as part of the hypothesis function\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        J : float\n",
    "            The cost of this given prediction\n",
    "        grad : float\n",
    "            The regularized gradient for this step of gradient descent\n",
    "    \"\"\"\n",
    "    # Number of training examples\n",
    "    m = y.size\n",
    "    # Perform sigmoid on hypothesis function\n",
    "    h = custom_ai_utils.sigmoid((np.power(X,powers).dot(theta.T)))    \n",
    "    # Just the regularized cost function for logistic regression, nothing to see here\n",
    "    J = (1/m)*np.sum((-y*np.log(h)) - (1-y)*(np.log(1-h))) + (lambda_/(2*m))*np.sum(np.square(theta))\n",
    "    # Set bias weight of theta to 0 because we don't want it to be regularized \n",
    "    temp = theta\n",
    "    temp[0] = 0\n",
    "    # Compute regularized gradient update \n",
    "    grad = (1/m)*((h-y).dot(X)) + (lambda_/m)*temp\n",
    "\n",
    "    return J, grad\n",
    "\n",
    "def costGradFunction(theta, X, y, powers):\n",
    "    \"\"\"\n",
    "        Same as above, but does not regularize\n",
    "    \"\"\"\n",
    "   \n",
    "    m = y.size\n",
    "   \n",
    "    h = custom_ai_utils.sigmoid(X.dot(theta.T))\n",
    "    \n",
    "    J = (1/m)*np.sum((-y*np.log(h)) - (1-y)*(np.log(1-h)))    \n",
    "    \n",
    "    grad = (1/m)*((h-y).dot(X)) \n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneVsAllReg(X, y, num_classes, lambda_, powers):\n",
    "    \"\"\"\n",
    "        Perform regularized one vs. all classification \n",
    "        \n",
    "        Determine the optimal weights for each class with the given data\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array (n,6)\n",
    "            The feature vector, containing n examples and 6 features including the intercept term\n",
    "        y : numpy array (n,)\n",
    "            The output vector, containing actual votes for each of the examples\n",
    "        num_classes : int\n",
    "            The number of possible prediction classes\n",
    "        lambda_ : float\n",
    "            The regularization parameter\n",
    "        powers : array (6,)\n",
    "            The exponents to raise each feature to in X, as part of the hypothesis function\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        all_theta : numpy array (num_classes,n)\n",
    "            The weight matrix, containing optimized weights for each class\n",
    "    \"\"\"\n",
    "    # Number of training examples, number of features (including intercept term)\n",
    "    m, n = X.shape\n",
    "    # Initialize weight matrix \n",
    "    all_theta = np.zeros((num_classes, n))\n",
    "    # Assign optimal theta for each class \n",
    "    for c in range(num_classes):\n",
    "        initial_theta = np.zeros(n)\n",
    "        options = {'maxiter' : 50} # Feel free to play around with this \n",
    "        res = optimize.minimize(costGradRegFunction, initial_theta, (X, (y==c)*1, lambda_, powers),\n",
    "                               jac=True, method='TNC', options=options) #Change extra params mayhaps\n",
    "        all_theta[c] = res.x\n",
    "    \n",
    "    return all_theta\n",
    "\n",
    "def oneVsAll(X, y, num_classes, powers):\n",
    "    \"\"\"\n",
    "        Same as above, but unregularized\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    all_theta = np.zeros((num_classes, n))\n",
    "    for c in range(num_classes):\n",
    "        initial_theta = np.zeros(n)\n",
    "        options = {'maxiter' : 50}\n",
    "        res = optimize.minimize(costGradFunction, initial_theta, (X, (y==c)*1, powers),\n",
    "                               jac=True, method='TNC', options=options)\n",
    "        all_theta[c] = res.x\n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Given optimized theta for each class (all_theta) and training data (X), return all \n",
    "    classifications for each element in X. \n",
    "\"\"\"\n",
    "def classifyOneVsAll(all_theta, X, powers):\n",
    "    # Will always be 3 in this case but whatever. Nice to seem generalized. \n",
    "    num_labels = np.array(all_theta.shape[0])\n",
    "    \n",
    "    # Array of predictions to return\n",
    "    p = np.zeros(X.shape[0])\n",
    "    \n",
    "    # Perform predictions with theta and X for each possible class \n",
    "    predictions = custom_ai_utils.sigmoid(np.power(X, powers).dot(all_theta.T))\n",
    "    p = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 79.71%\n"
     ]
    }
   ],
   "source": [
    "# Some random testing things\n",
    "lambda_ = .1\n",
    "num_classes = 3\n",
    "theta = np.ones(6)\n",
    "h = custom_ai_utils.sigmoid(X.dot(theta.T))\n",
    "powers = np.ones(6)\n",
    "\n",
    "#all_theta = oneVsAllReg(X, y, num_classes, lambda_, powers)\n",
    "all_theta = oneVsAll(X, y, num_classes, powers)\n",
    "predictions = classifyOneVsAll(all_theta, X, powers)\n",
    "\n",
    "print('Training Set Accuracy: {:.2f}%'.format(np.mean(predictions == y) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main: 0\n",
      "Sub: 0\n",
      "Current best_theta: \n",
      "Sub: 1\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 2\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 3\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 4\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 5\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Main: 1\n",
      "Sub: 0\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 1\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 2\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 3\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 4\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 5\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Main: 2\n",
      "Sub: 0\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 1\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 2\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 3\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 4\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 5\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Main: 3\n",
      "Sub: 0\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 1\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 2\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 3\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 4\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 5\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Main: 4\n",
      "Sub: 0\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 1\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 2\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 3\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 4\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 5\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Main: 5\n",
      "Sub: 0\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 1\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 2\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 3\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub: 4\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Sub: 5\n",
      "Current best_theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Best hypothesis function has powers: [0 2 4 4 0 0]\n",
      "It had the following performance : 82.6086956522\n",
      "It had the following theta: [[-0.94707612  0.0935797   0.21562864 -0.06362213 -0.0705111  -1.06748832]\n",
      " [ 0.37619398 -0.10531447 -0.06600369  0.07863234  0.0143614   1.69845091]\n",
      " [-2.51165427  0.11899932 -1.59840977 -0.13111087  0.21506235 -2.90024353]]\n",
      "Time took: 300.63893485069275\n"
     ]
    }
   ],
   "source": [
    "# Trying to identify best hypothesis by evaluating every single possible combination\n",
    "best_powers = ''\n",
    "best_performance = 0\n",
    "best_theta = ''\n",
    "lambda_ = .1\n",
    "num_classes = 3\n",
    "now = time.time()\n",
    "theta = np.ones(6)\n",
    "for i in range(6):\n",
    "    print(\"Main: \" + str(i))\n",
    "    for j in range(6):\n",
    "        print(\"Sub: \" + str(j))\n",
    "        print(\"Current best_theta: \" + str(best_theta))\n",
    "        for k in range(6):\n",
    "            for l in range(6):\n",
    "                for m in range(6):\n",
    "                    for n in range(6):\n",
    "                        powers = np.array([i,j,k,l,m,n])\n",
    "                        #all_theta = oneVsAllReg(X, y, num_classes, lambda_, powers)\n",
    "                        all_theta = oneVsAll(X, y, num_classes, powers)\n",
    "                        preds = classifyOneVsAll(all_theta, X, powers)\n",
    "                        performance = np.mean(preds == y) * 100\n",
    "                        if(performance > best_performance):\n",
    "                            best_performance = performance\n",
    "                            best_powers = powers\n",
    "                            best_theta = all_theta\n",
    "then = time.time()\n",
    "print(\"Best hypothesis function has powers: \" + str(best_powers))\n",
    "print(\"It had the following performance : \" + str(best_performance))\n",
    "print(\"It had the following theta: \" + str(best_theta))\n",
    "print(\"Time took: \" + str(then - now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check out [my website](https://dylanpjackson.com) please <3 and maybe hire me if you want"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
