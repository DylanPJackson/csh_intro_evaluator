{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict an applicant's outcome when they apply to CSH using past data. Employs logistic regression to identify potential outcome classes. Check out the README for a better explanation.  \n",
    "        \n",
    "# By : Dylan P. Jackson\n",
    "check out [my website](https://dylanpjackson.com) please <3 and maybe hire me if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Them imports yo (Will probably need more)\n",
    "import numpy as np\n",
    "import sys\n",
    "from scipy import optimize\n",
    "sys.path.insert(1,\"../ai_implementations\")\n",
    "import custom_ai_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in some data\n",
    "data_path = \"data/intro.csv\"\n",
    "data = np.loadtxt(data_path, delimiter = \",\", usecols = (1,2,3,4,5,6))\n",
    "\n",
    "# Partition training data and result data \n",
    "X = data[:,:5]\n",
    "y = data[:,5]\n",
    "m = y.size\n",
    "\n",
    "# Add the intercept term\n",
    "X = np.concatenate([np.ones((m, 1)), X], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradient descent for this boi will be done using scipy.optimize\n",
    "# To that end, we establish a cost function which not only returns the cost\n",
    "# but the gradient at each step as well \n",
    "\n",
    "\"\"\"\n",
    "    Given training data (X), result data (y), theta (theta), regularization parameter (lambda_)\n",
    "    and hypothesis function which was computed with given X and theta (h), return the cost and\n",
    "    gradient of hypothesis function with the given theta \n",
    "\"\"\"\n",
    "def costGradFunction(theta, X, y, lambda_):\n",
    "    # Number of training examples\n",
    "    m = y.size\n",
    "    \n",
    "    h = custom_ai_utils.sigmoid(X.dot(theta.T))\n",
    "    \n",
    "    # Yikes. Really, it's just the regularized cost function for logistic regression. \n",
    "    J = (1/m)*np.sum((-y*np.log(h)) - (1-y)*(np.log(1-h))) + (lambda_/(2*m))*np.sum(np.square(theta))\n",
    "    \n",
    "    # Set bias weight of theta to 0 because we don't want it to be regularized \n",
    "    temp = theta\n",
    "    temp[0] = 0\n",
    "    \n",
    "    # Compute regularized gradient update \n",
    "    grad = (1/m)*((h-y).dot(X)) + (lambda_/m)*temp\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Given training data (X), result data (y), the number of classes (num_classes) and \n",
    "    regularization parameter (lambda_), return optimized theta for each class\n",
    "\"\"\"\n",
    "def oneVsAll(X, y, num_classes, lambda_):\n",
    "    # Number of training examples, number of features (including intercept term)\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initialize theta matrix \n",
    "    all_theta = np.zeros((num_classes, n))\n",
    "    # Assign optimal theta for each class \n",
    "    for c in range(num_classes):\n",
    "        initial_theta = np.zeros(n)\n",
    "        options = {'maxiter' : 50} # Feel free to play around with this \n",
    "        res = optimize.minimize(costGradFunction, initial_theta, (X, (y==c)*1, lambda_),\n",
    "                               jac=True, method='TNC', options=options) #Change extra params mayhaps\n",
    "        all_theta[c] = res.x\n",
    "    \n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Given optimized theta for each class (all_theta) and training data (X), return all \n",
    "    classifications for each element in X. \n",
    "\"\"\"\n",
    "def classifyOneVsAll(all_theta, X):\n",
    "    # Will always be 3 in this case but whatever. Nice to seem generalized. \n",
    "    num_labels = np.array(all_theta.shape[0])\n",
    "    \n",
    "    # Array of predictions to return\n",
    "    p = np.zeros(X.shape[0])\n",
    "    \n",
    "    # Perform predictions with theta and X for each possible class \n",
    "    predictions = custom_ai_utils.sigmoid(X.dot(all_theta.T))\n",
    "    p = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 80.19%\n"
     ]
    }
   ],
   "source": [
    "lambda_ = .1\n",
    "num_classes = 3\n",
    "theta = np.ones(6)\n",
    "h = custom_ai_utils.sigmoid(X.dot(theta.T))\n",
    "\n",
    "all_theta = oneVsAll(X, y, num_classes, lambda_)\n",
    "predictions = classifyOneVsAll(all_theta, X)\n",
    "\n",
    "print('Training Set Accuracy: {:.2f}%'.format(np.mean(predictions == y) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
